//! PTX Module and Kernel Builder
//!
//! Provides a fluent builder API for constructing PTX modules and kernels.

use std::fmt::Write;

use super::instructions::{CmpOp, Operand, Predicate, PtxInstruction, PtxOp, RoundingMode, WmmaLayout};
use super::registers::{PtxReg, RegisterAllocator, VirtualReg};
use super::types::{PtxStateSpace, PtxType};
use super::{validate_target, validate_version};
use crate::error::Result;

/// PTX Module builder
#[derive(Debug, Clone)]
pub struct PtxModule {
    /// PTX version (major, minor)
    version: (u32, u32),
    /// Target compute capability (e.g., `sm_70`)
    target: String,
    /// Address size (32 or 64)
    address_size: u32,
    /// Kernels in this module
    kernels: Vec<PtxKernel>,
}

impl PtxModule {
    /// Create a new PTX module with defaults
    #[must_use]
    pub fn new() -> Self {
        Self {
            version: (8, 0),
            target: "sm_70".to_string(),
            address_size: 64,
            kernels: Vec::new(),
        }
    }

    /// Set PTX version
    #[must_use]
    pub fn version(mut self, major: u32, minor: u32) -> Self {
        self.version = (major, minor);
        self
    }

    /// Get PTX version
    #[must_use]
    pub const fn get_version(&self) -> (u32, u32) {
        self.version
    }

    /// Set target compute capability
    #[must_use]
    pub fn target(mut self, target: impl Into<String>) -> Self {
        self.target = target.into();
        self
    }

    /// Get target
    #[must_use]
    pub fn get_target(&self) -> &str {
        &self.target
    }

    /// Set address size
    #[must_use]
    pub const fn address_size(mut self, size: u32) -> Self {
        self.address_size = size;
        self
    }

    /// Get address size
    #[must_use]
    pub const fn get_address_size(&self) -> u32 {
        self.address_size
    }

    /// Add a kernel to the module
    #[must_use]
    pub fn add_kernel(mut self, kernel: PtxKernel) -> Self {
        self.kernels.push(kernel);
        self
    }

    /// Validate the module configuration
    ///
    /// # Errors
    ///
    /// Returns an error if:
    /// - The PTX version is below the minimum supported (7.0)
    /// - The target compute capability is invalid
    pub fn validate(&self) -> Result<()> {
        validate_version(self.version.0, self.version.1)?;
        validate_target(&self.target)?;
        Ok(())
    }

    /// Emit PTX source code
    #[must_use]
    pub fn emit(&self) -> String {
        let mut ptx = String::new();

        // Header comment
        ptx.push_str("// Generated by trueno-gpu\n");
        ptx.push_str("// Pure Rust PTX generation - no external dependencies\n\n");

        // Version directive
        let _ = writeln!(ptx, ".version {}.{}", self.version.0, self.version.1);

        // Target directive
        let _ = writeln!(ptx, ".target {}", self.target);

        // Address size directive
        let _ = writeln!(ptx, ".address_size {}\n", self.address_size);

        // Emit each kernel
        for kernel in &self.kernels {
            ptx.push_str(&kernel.emit());
            ptx.push('\n');
        }

        ptx
    }
}

impl Default for PtxModule {
    fn default() -> Self {
        Self::new()
    }
}

/// Kernel parameter
#[derive(Debug, Clone)]
pub struct KernelParam {
    /// Parameter type
    pub ty: PtxType,
    /// Parameter name
    pub name: String,
}

/// PTX Kernel builder
#[derive(Debug, Clone)]
pub struct PtxKernel {
    /// Kernel name
    name: String,
    /// Parameters
    params: Vec<KernelParam>,
    /// Shared memory size in bytes
    shared_memory: usize,
    /// Instructions
    instructions: Vec<PtxInstruction>,
    /// Register allocator
    registers: RegisterAllocator,
    /// Labels
    labels: Vec<String>,
}

impl PtxKernel {
    /// Create a new kernel
    #[must_use]
    pub fn new(name: impl Into<String>) -> Self {
        Self {
            name: name.into(),
            params: Vec::new(),
            shared_memory: 0,
            instructions: Vec::new(),
            registers: RegisterAllocator::new(),
            labels: Vec::new(),
        }
    }

    /// Add a parameter
    #[must_use]
    pub fn param(mut self, ty: PtxType, name: impl Into<String>) -> Self {
        self.params.push(KernelParam {
            ty,
            name: name.into(),
        });
        self
    }

    /// Set shared memory size
    #[must_use]
    pub const fn shared_memory(mut self, bytes: usize) -> Self {
        self.shared_memory = bytes;
        self
    }

    /// Get shared memory size
    #[must_use]
    pub const fn shared_memory_bytes(&self) -> usize {
        self.shared_memory
    }

    /// Build kernel body with a closure
    #[must_use]
    pub fn build<F>(mut self, builder_fn: F) -> Self
    where
        F: FnOnce(&mut KernelBuilder<'_>),
    {
        let mut builder = KernelBuilder::new(&mut self.registers);
        builder_fn(&mut builder);
        self.instructions = builder.instructions;
        self.labels = builder.labels;
        self
    }

    /// Emit kernel PTX
    #[must_use]
    pub fn emit(&self) -> String {
        use std::fmt::Write;
        let mut ptx = String::new();

        // Kernel entry point
        let _ = writeln!(ptx, ".visible .entry {}(", self.name);

        // Parameters
        for (i, param) in self.params.iter().enumerate() {
            let comma = if i < self.params.len() - 1 { "," } else { "" };
            let _ = writeln!(
                ptx,
                "    .param {} {}{}",
                param.ty.to_ptx_string(),
                param.name,
                comma
            );
        }

        ptx.push_str(") {\n");

        // Register declarations
        ptx.push_str(&self.registers.emit_declarations());

        // Shared memory declaration (if any)
        if self.shared_memory > 0 {
            let _ = writeln!(
                ptx,
                "    .shared .align 16 .b8 smem[{}];",
                self.shared_memory
            );
        }

        ptx.push('\n');

        // Instructions
        for instr in &self.instructions {
            ptx.push_str(&emit_instruction(instr));
        }

        ptx.push_str("}\n");
        ptx
    }
}

/// Kernel builder context (passed to build closure)
pub struct KernelBuilder<'a> {
    /// Register allocator
    registers: &'a mut RegisterAllocator,
    /// Instructions
    instructions: Vec<PtxInstruction>,
    /// Labels
    labels: Vec<String>,
}

impl<'a> KernelBuilder<'a> {
    fn new(registers: &'a mut RegisterAllocator) -> Self {
        Self {
            registers,
            instructions: Vec::new(),
            labels: Vec::new(),
        }
    }

    // ===== Special Registers =====

    /// Read a special register into a virtual register
    pub fn special_reg(&mut self, reg: PtxReg) -> VirtualReg {
        let vreg = self.registers.allocate_virtual(reg.data_type());
        self.instructions.push(
            PtxInstruction::new(PtxOp::Mov, reg.data_type())
                .dst(Operand::Reg(vreg))
                .src(Operand::SpecialReg(reg)),
        );
        vreg
    }

    // ===== Parameter Loading =====

    /// Load a u32 parameter
    pub fn load_param_u32(&mut self, name: &str) -> VirtualReg {
        let vreg = self.registers.allocate_virtual(PtxType::U32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::LdParam, PtxType::U32)
                .dst(Operand::Reg(vreg))
                .src(Operand::Param(name.to_string())),
        );
        vreg
    }

    /// Load a u64 parameter
    pub fn load_param_u64(&mut self, name: &str) -> VirtualReg {
        let vreg = self.registers.allocate_virtual(PtxType::U64);
        self.instructions.push(
            PtxInstruction::new(PtxOp::LdParam, PtxType::U64)
                .dst(Operand::Reg(vreg))
                .src(Operand::Param(name.to_string())),
        );
        vreg
    }

    // ===== Arithmetic =====

    /// Multiply-add low: dst = a * b + c
    pub fn mad_lo_u32(&mut self, a: VirtualReg, b: VirtualReg, c: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::MadLo, PtxType::U32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::Reg(b))
                .src(Operand::Reg(c)),
        );
        dst
    }

    /// Multiply wide (u32 * u32 -> u64)
    pub fn mul_wide_u32(&mut self, a: VirtualReg, b: u32) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U64);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Mul, PtxType::U64)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::ImmU64(b as u64)),
        );
        dst
    }

    /// Multiply wide (u32 * u32 -> u64) with register operands
    pub fn mul_wide_u32_reg(&mut self, a: VirtualReg, b: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U64);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Mul, PtxType::U64)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::Reg(b)),
        );
        dst
    }

    /// Add u64
    pub fn add_u64(&mut self, a: VirtualReg, b: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U64);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Add, PtxType::U64)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::Reg(b)),
        );
        dst
    }

    /// Add f32
    pub fn add_f32(&mut self, a: VirtualReg, b: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Add, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::Reg(b))
                .rounding(RoundingMode::Rn),
        );
        dst
    }

    // ===== Comparison =====

    /// Set predicate if a >= b (unsigned)
    pub fn setp_ge_u32(&mut self, a: VirtualReg, b: VirtualReg) -> VirtualReg {
        let pred = self.registers.allocate_virtual(PtxType::Pred);
        let mut instr = PtxInstruction::new(PtxOp::Setp, PtxType::U32)
            .dst(Operand::Reg(pred))
            .src(Operand::Reg(a))
            .src(Operand::Reg(b));
        // Comparison operator stored in label field (emitted as setp.ge.u32)
        instr.label = Some(CmpOp::Ge.to_ptx_string().to_string());
        self.instructions.push(instr);
        pred
    }

    // ===== Memory Operations =====

    /// Load f32 from global memory
    pub fn ld_global_f32(&mut self, addr: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Ld, PtxType::F32)
                .space(PtxStateSpace::Global)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(addr)),
        );
        dst
    }

    /// Store f32 to global memory
    pub fn st_global_f32(&mut self, addr: VirtualReg, val: VirtualReg) {
        self.instructions.push(
            PtxInstruction::new(PtxOp::St, PtxType::F32)
                .space(PtxStateSpace::Global)
                .src(Operand::Reg(addr))
                .src(Operand::Reg(val)),
        );
    }

    // ===== Control Flow =====

    /// Branch if predicate is true
    pub fn branch_if(&mut self, pred: VirtualReg, label: &str) {
        let predicate = Predicate {
            reg: pred,
            negated: false,
        };
        self.instructions.push(
            PtxInstruction::new(PtxOp::Bra, PtxType::B32)
                .predicated(predicate)
                .label(label),
        );
    }

    /// Define a label
    pub fn label(&mut self, name: &str) {
        self.labels.push(name.to_string());
        // Labels are emitted inline, store as special instruction
        let mut instr = PtxInstruction::new(PtxOp::Mov, PtxType::B32);
        instr.label = Some(format!("{}:", name));
        self.instructions.push(instr);
    }

    /// Return from kernel
    pub fn ret(&mut self) {
        self.instructions
            .push(PtxInstruction::new(PtxOp::Ret, PtxType::B32));
    }

    /// Unconditional branch
    pub fn branch(&mut self, label: &str) {
        self.instructions.push(
            PtxInstruction::new(PtxOp::Bra, PtxType::B32).label(label),
        );
    }

    // ===== Immediate Moves =====

    /// Move immediate f32 value
    pub fn mov_f32_imm(&mut self, val: f32) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Mov, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::ImmF32(val)),
        );
        dst
    }

    /// Move immediate u32 value
    pub fn mov_u32_imm(&mut self, val: u32) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Mov, PtxType::U32)
                .dst(Operand::Reg(dst))
                .src(Operand::ImmU64(val as u64)),
        );
        dst
    }

    // ===== Additional Arithmetic =====

    /// Multiply f32
    pub fn mul_f32(&mut self, a: VirtualReg, b: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Mul, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::Reg(b))
                .rounding(RoundingMode::Rn),
        );
        dst
    }

    /// Add u32
    pub fn add_u32(&mut self, a: VirtualReg, b: u32) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Add, PtxType::U32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::ImmU64(b as u64)),
        );
        dst
    }

    /// Fused multiply-add f32: dst = a * b + c
    pub fn fma_f32(&mut self, a: VirtualReg, b: VirtualReg, c: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Fma, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::Reg(b))
                .src(Operand::Reg(c))
                .rounding(RoundingMode::Rn),
        );
        dst
    }

    /// Barrier synchronization (all threads in block must reach this point)
    pub fn bar_sync(&mut self, barrier_id: u32) {
        self.instructions.push(
            PtxInstruction::new(PtxOp::Bar, PtxType::B32).label(format!("sync {}", barrier_id)),
        );
    }

    /// Load from shared memory
    pub fn ld_shared_f32(&mut self, addr: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Ld, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(addr))
                .space(PtxStateSpace::Shared),
        );
        dst
    }

    /// Store to shared memory
    pub fn st_shared_f32(&mut self, addr: VirtualReg, val: VirtualReg) {
        self.instructions.push(
            PtxInstruction::new(PtxOp::St, PtxType::F32)
                .src(Operand::Reg(addr))
                .src(Operand::Reg(val))
                .space(PtxStateSpace::Shared),
        );
    }

    /// Warp shuffle down (for reductions)
    /// Format: shfl.sync.down.b32 dst, src, delta, clamp, membermask
    pub fn shfl_down_f32(&mut self, val: VirtualReg, offset: u32, mask: u32) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::ShflDown, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(val))
                .src(Operand::ImmU64(offset as u64))
                .src(Operand::ImmU64(31)) // clamp to warp size
                .src(Operand::ImmU64(mask as u64)), // membermask
        );
        dst
    }

    /// Warp shuffle indexed (for broadcasts - gets value from specific lane)
    /// Format: shfl.sync.idx.b32 dst, src, srcLane, clamp, membermask
    pub fn shfl_idx_f32(&mut self, val: VirtualReg, src_lane: u32, mask: u32) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::ShflIdx, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(val))
                .src(Operand::ImmU64(src_lane as u64))
                .src(Operand::ImmU64(31)) // clamp to warp size
                .src(Operand::ImmU64(mask as u64)), // membermask
        );
        dst
    }

    /// Max f32 of two values
    pub fn max_f32(&mut self, a: VirtualReg, b: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Max, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::Reg(b)),
        );
        dst
    }

    /// Exp f32 (exponential)
    pub fn ex2_f32(&mut self, val: VirtualReg) -> VirtualReg {
        // PTX has ex2 (base 2), we scale input by log2(e) for natural exp
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Ex2, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(val)),
        );
        dst
    }

    /// Sub f32
    pub fn sub_f32(&mut self, a: VirtualReg, b: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Sub, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::Reg(b)),
        );
        dst
    }

    /// Div f32
    pub fn div_f32(&mut self, a: VirtualReg, b: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Div, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::Reg(b)),
        );
        dst
    }

    /// Setp less than u32
    pub fn setp_lt_u32(&mut self, a: VirtualReg, b: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::Pred);
        let mut instr = PtxInstruction::new(PtxOp::Setp, PtxType::U32)
            .dst(Operand::Reg(dst))
            .src(Operand::Reg(a))
            .src(Operand::Reg(b));
        // Comparison operator stored in label field (emitted as setp.lt.u32)
        instr.label = Some(CmpOp::Lt.to_ptx_string().to_string());
        self.instructions.push(instr);
        dst
    }

    /// Multiply u32
    pub fn mul_u32(&mut self, a: VirtualReg, b: u32) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Mul, PtxType::U32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::ImmU64(b as u64)),
        );
        dst
    }

    /// Multiply u32 (register * register)
    pub fn mul_u32_reg(&mut self, a: VirtualReg, b: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Mul, PtxType::U32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::Reg(b)),
        );
        dst
    }

    /// Add u32 (register + register)
    pub fn add_u32_reg(&mut self, a: VirtualReg, b: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Add, PtxType::U32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::Reg(b)),
        );
        dst
    }

    /// Convert u32 to u64 (zero extend)
    pub fn cvt_u64_u32(&mut self, val: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U64);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Cvt, PtxType::U64)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(val)),
        );
        dst
    }

    /// Convert u32 to f32
    pub fn cvt_f32_u32(&mut self, val: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Cvt, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(val))
                .rounding(RoundingMode::Rn),
        );
        dst
    }

    /// Reciprocal square root f32: dst = 1/sqrt(val)
    pub fn rsqrt_f32(&mut self, val: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Rsqrt, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(val)),
        );
        dst
    }

    /// Integer division u32
    pub fn div_u32(&mut self, a: VirtualReg, b: u32) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Div, PtxType::U32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::ImmU64(b as u64)),
        );
        dst
    }

    /// Integer remainder (modulo) u32
    pub fn rem_u32(&mut self, a: VirtualReg, b: u32) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Rem, PtxType::U32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::ImmU64(b as u64)),
        );
        dst
    }

    /// Move immediate u64 value
    pub fn mov_u64_imm(&mut self, val: u64) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U64);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Mov, PtxType::U64)
                .dst(Operand::Reg(dst))
                .src(Operand::ImmU64(val)),
        );
        dst
    }

    /// Multiply u64 by immediate
    pub fn mul_u64(&mut self, a: VirtualReg, b: u64) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U64);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Mul, PtxType::U64)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::ImmU64(b)),
        );
        dst
    }

    /// Multiply u64 (register * register)
    pub fn mul_u64_reg(&mut self, a: VirtualReg, b: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U64);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Mul, PtxType::U64)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::Reg(b)),
        );
        dst
    }

    /// Branch if predicate is false (negated predicate)
    pub fn branch_if_not(&mut self, pred: VirtualReg, label: &str) {
        let predicate = Predicate {
            reg: pred,
            negated: true,
        };
        self.instructions.push(
            PtxInstruction::new(PtxOp::Bra, PtxType::B32)
                .predicated(predicate)
                .label(label),
        );
    }

    /// Load u32 from global memory
    pub fn ld_global_u32(&mut self, addr: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Ld, PtxType::U32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(addr))
                .space(PtxStateSpace::Global),
        );
        dst
    }

    /// Load u8 from global memory
    pub fn ld_global_u8(&mut self, addr: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U8);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Ld, PtxType::U8)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(addr))
                .space(PtxStateSpace::Global),
        );
        dst
    }

    /// Convert u8 to u32 (zero extend)
    pub fn cvt_u32_u8(&mut self, val: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Cvt, PtxType::U32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(val)),
        );
        dst
    }

    /// Shift right u32 (logical shift)
    pub fn shr_u32(&mut self, val: VirtualReg, shift: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Shr, PtxType::U32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(val))
                .src(Operand::Reg(shift)),
        );
        dst
    }

    /// Bitwise AND u32 (register AND register)
    pub fn and_u32(&mut self, a: VirtualReg, b: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::U32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::And, PtxType::U32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::Reg(b)),
        );
        dst
    }

    // ===== In-Place Updates (for loops) =====

    /// Add u32 immediate in-place: dst = dst + imm
    /// Used for loop counter updates where SSA would allocate a new register
    pub fn add_u32_inplace(&mut self, dst: VirtualReg, imm: u32) {
        self.registers.extend_live_range(dst);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Add, PtxType::U32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(dst))
                .src(Operand::ImmU64(imm as u64)),
        );
    }

    /// Add f32 register in-place: dst = dst + src
    /// Used for accumulator updates in reduction loops
    pub fn add_f32_inplace(&mut self, dst: VirtualReg, src: VirtualReg) {
        self.registers.extend_live_range(dst);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Add, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(dst))
                .src(Operand::Reg(src))
                .rounding(RoundingMode::Rn),
        );
    }

    /// Fused multiply-add in-place: dst = a * b + dst
    /// Used for GEMM accumulation
    pub fn fma_f32_inplace(&mut self, dst: VirtualReg, a: VirtualReg, b: VirtualReg) {
        self.registers.extend_live_range(dst);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Fma, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(a))
                .src(Operand::Reg(b))
                .src(Operand::Reg(dst))
                .rounding(RoundingMode::Rn),
        );
    }

    /// Max in-place: dst = max(dst, src)
    /// Used for online softmax running max
    pub fn max_f32_inplace(&mut self, dst: VirtualReg, src: VirtualReg) {
        self.registers.extend_live_range(dst);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Max, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(dst))
                .src(Operand::Reg(src)),
        );
    }

    /// Copy f32 register: dst = src
    /// Used for accumulator state updates
    pub fn mov_f32_reg(&mut self, dst: VirtualReg, src: VirtualReg) {
        self.registers.extend_live_range(dst);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Mov, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(src)),
        );
    }

    /// Multiply in-place: dst = dst * src
    /// Used for scaling operations
    pub fn mul_f32_inplace(&mut self, dst: VirtualReg, src: VirtualReg) {
        self.registers.extend_live_range(dst);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Mul, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(dst))
                .src(Operand::Reg(src))
                .rounding(RoundingMode::Rn),
        );
    }

    /// Divide in-place: dst = dst / src
    /// Used for normalization
    pub fn div_f32_inplace(&mut self, dst: VirtualReg, src: VirtualReg) {
        self.registers.extend_live_range(dst);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Div, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(dst))
                .src(Operand::Reg(src))
                .rounding(RoundingMode::Rn),
        );
    }

    // ===== Tensor Core (WMMA) Operations =====
    // These require sm_70+ and generate WMMA PTX intrinsics

    /// Load F16 matrix fragment A for WMMA (16x16x16 tile)
    /// Returns fragment registers for use in wmma_mma
    pub fn wmma_load_a_f16(
        &mut self,
        addr: VirtualReg,
        stride: u32,
        layout: WmmaLayout,
    ) -> Vec<VirtualReg> {
        // WMMA 16x16x16 F16 requires 8 F16x2 registers (16 half values)
        let mut frag = Vec::with_capacity(8);
        for _ in 0..8 {
            frag.push(self.registers.allocate_virtual(PtxType::B32));
        }
        self.instructions.push(
            PtxInstruction::new(PtxOp::WmmaLoadA, PtxType::F16)
                .dst(Operand::Reg(frag[0]))
                .src(Operand::Reg(addr))
                .label(format!("m16n16k16.{}.f16.stride.{}", layout.to_ptx_string(), stride)),
        );
        frag
    }

    /// Load F16 matrix fragment B for WMMA (16x16x16 tile)
    pub fn wmma_load_b_f16(
        &mut self,
        addr: VirtualReg,
        stride: u32,
        layout: WmmaLayout,
    ) -> Vec<VirtualReg> {
        let mut frag = Vec::with_capacity(8);
        for _ in 0..8 {
            frag.push(self.registers.allocate_virtual(PtxType::B32));
        }
        self.instructions.push(
            PtxInstruction::new(PtxOp::WmmaLoadB, PtxType::F16)
                .dst(Operand::Reg(frag[0]))
                .src(Operand::Reg(addr))
                .label(format!("m16n16k16.{}.f16.stride.{}", layout.to_ptx_string(), stride)),
        );
        frag
    }

    /// Load F32 accumulator fragment C for WMMA (16x16x16 tile)
    pub fn wmma_load_c_f32(
        &mut self,
        addr: VirtualReg,
        stride: u32,
        layout: WmmaLayout,
    ) -> Vec<VirtualReg> {
        // Accumulator is 8 F32 values
        let mut frag = Vec::with_capacity(8);
        for _ in 0..8 {
            frag.push(self.registers.allocate_virtual(PtxType::F32));
        }
        self.instructions.push(
            PtxInstruction::new(PtxOp::WmmaLoadC, PtxType::F32)
                .dst(Operand::Reg(frag[0]))
                .src(Operand::Reg(addr))
                .label(format!("m16n16k16.{}.f32.stride.{}", layout.to_ptx_string(), stride)),
        );
        frag
    }

    /// WMMA matrix multiply-accumulate: D = A * B + C
    /// Takes A, B, C fragment registers and returns D fragment registers
    #[allow(clippy::similar_names)]
    pub fn wmma_mma_f16_f32(
        &mut self,
        frag_a: &[VirtualReg],
        frag_b: &[VirtualReg],
        frag_c: &[VirtualReg],
    ) -> Vec<VirtualReg> {
        // Output accumulator D (8 F32 values)
        let mut frag_d = Vec::with_capacity(8);
        for _ in 0..8 {
            frag_d.push(self.registers.allocate_virtual(PtxType::F32));
        }

        // MMA instruction references all fragment registers
        let mut instr = PtxInstruction::new(PtxOp::WmmaMma, PtxType::F32)
            .dst(Operand::Reg(frag_d[0]))
            .label("m16n16k16.row.col.f32.f32");

        // Add A, B, C fragment sources
        if !frag_a.is_empty() {
            instr = instr.src(Operand::Reg(frag_a[0]));
        }
        if !frag_b.is_empty() {
            instr = instr.src(Operand::Reg(frag_b[0]));
        }
        if !frag_c.is_empty() {
            instr = instr.src(Operand::Reg(frag_c[0]));
        }

        self.instructions.push(instr);
        frag_d
    }

    /// Store F32 accumulator fragment D to memory
    pub fn wmma_store_d_f32(
        &mut self,
        addr: VirtualReg,
        frag_d: &[VirtualReg],
        stride: u32,
        layout: WmmaLayout,
    ) {
        if frag_d.is_empty() {
            return;
        }
        self.instructions.push(
            PtxInstruction::new(PtxOp::WmmaStoreD, PtxType::F32)
                .src(Operand::Reg(addr))
                .src(Operand::Reg(frag_d[0]))
                .label(format!("m16n16k16.{}.f32.stride.{}", layout.to_ptx_string(), stride)),
        );
    }

    /// Convert F32 values to F16 (for feeding tensor cores)
    pub fn cvt_f16_f32(&mut self, val: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F16);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Cvt, PtxType::F16)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(val))
                .rounding(RoundingMode::Rn),
        );
        dst
    }

    /// Convert F16 value to F32 (for accumulation)
    pub fn cvt_f32_f16(&mut self, val: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F32);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Cvt, PtxType::F32)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(val)),
        );
        dst
    }

    /// Load F16 from global memory
    pub fn ld_global_f16(&mut self, addr: VirtualReg) -> VirtualReg {
        let dst = self.registers.allocate_virtual(PtxType::F16);
        self.instructions.push(
            PtxInstruction::new(PtxOp::Ld, PtxType::F16)
                .dst(Operand::Reg(dst))
                .src(Operand::Reg(addr))
                .space(PtxStateSpace::Global),
        );
        dst
    }

    /// Store F16 to global memory
    pub fn st_global_f16(&mut self, addr: VirtualReg, val: VirtualReg) {
        self.instructions.push(
            PtxInstruction::new(PtxOp::St, PtxType::F16)
                .src(Operand::Reg(addr))
                .src(Operand::Reg(val))
                .space(PtxStateSpace::Global),
        );
    }
}

/// Emit a single instruction as PTX
#[allow(clippy::too_many_lines)]
fn emit_instruction(instr: &PtxInstruction) -> String {
    let mut s = String::new();

    // Handle labels
    if let Some(label) = &instr.label {
        if label.ends_with(':') {
            return format!("{}:\n", &label[..label.len() - 1]);
        }
    }

    // Predicate
    if let Some(pred) = &instr.predicate {
        let neg = if pred.negated { "!" } else { "" };
        s.push_str(&format!("    @{}{} ", neg, pred.reg.to_ptx_string()));
    } else {
        s.push_str("    ");
    }

    // Opcode
    match instr.op {
        PtxOp::Mov => s.push_str("mov"),
        PtxOp::Add => s.push_str("add"),
        PtxOp::Sub => s.push_str("sub"),
        PtxOp::Mul => {
            // Check for wide multiply (dest is 64-bit from 32-bit sources)
            // mul.wide.u32 produces u64 from u32 * u32
            // BUT if source operands are already u64, use mul.lo.u64 instead
            let is_wide_output = instr.ty == PtxType::U64 || instr.ty == PtxType::S64;
            let has_u64_source = instr.srcs.first().is_some_and(|src| {
                matches!(src, Operand::Reg(vreg) if vreg.ty() == PtxType::U64 || vreg.ty() == PtxType::S64)
            });

            if is_wide_output && !has_u64_source {
                // Wide multiply uses source type, not dest type
                let src_ty = if instr.ty == PtxType::U64 { ".u32" } else { ".s32" };
                s.push_str("mul.wide");
                s.push_str(src_ty);
            } else if is_wide_output && has_u64_source {
                // u64 * u64 -> u64: use mul.lo.u64
                s.push_str("mul.lo");
            } else if instr.ty.is_float() {
                // Floating point multiply (no .lo needed)
                s.push_str("mul");
            } else {
                // Integer multiply needs .lo to get low bits
                s.push_str("mul.lo");
            }
        }
        PtxOp::MadLo => s.push_str("mad.lo"),
        PtxOp::Div => {
            // Float div requires rounding mode, integer div doesn't
            if instr.ty.is_float() {
                s.push_str("div.rn");
            } else {
                s.push_str("div");
            }
        }
        PtxOp::Setp => {
            // Include comparison op from label
            let cmp = instr.label.as_deref().unwrap_or("eq");
            s.push_str(&format!("setp.{}", cmp));
        }
        PtxOp::Ld => {
            let space = instr
                .state_space
                .map(|ss| ss.to_ptx_string())
                .unwrap_or(".global");
            s.push_str(&format!("ld{}", space));
        }
        PtxOp::LdParam => s.push_str("ld.param"),
        PtxOp::St => {
            let space = instr
                .state_space
                .map(|ss| ss.to_ptx_string())
                .unwrap_or(".global");
            s.push_str(&format!("st{}", space));
        }
        PtxOp::Bra => {
            if let Some(label) = &instr.label {
                return format!("{}bra {};\n", s, label);
            }
            s.push_str("bra");
        }
        PtxOp::Ret => return format!("{}ret;\n", s),
        PtxOp::Bar => {
            // bar.sync instruction needs barrier ID from label
            let barrier_id = instr.label.as_deref().unwrap_or("sync 0");
            return format!("{}bar.{};\n", s, barrier_id);
        }
        PtxOp::Cvt => {
            // cvt needs both destination and source types
            // The source type is determined from the source operand
            let dst_ty = instr.ty.to_ptx_string();
            let src_ty = if let Some(Operand::Reg(vreg)) = instr.srcs.first() {
                vreg.ty().to_ptx_string()
            } else {
                ".u32" // Default fallback
            };
            // Add rounding mode for float conversions (required by PTX ISA)
            let needs_rounding = instr.ty.is_float()
                || instr.srcs.first().is_some_and(|src| {
                    matches!(src, Operand::Reg(vreg) if vreg.ty().is_float())
                });
            let round = if needs_rounding {
                instr.rounding.as_ref().map_or(".rn", |r| r.to_ptx_string())
            } else {
                ""
            };
            s.push_str(&format!("cvt{}{}{}", round, dst_ty, src_ty));
        }
        PtxOp::Fma => {
            // FMA requires rounding mode: fma.rn.f32
            let round = instr.rounding.as_ref().map_or(".rn", |r| r.to_ptx_string());
            s.push_str(&format!("fma{}", round));
        }
        PtxOp::ShflDown => {
            // sm_70+ requires shfl.sync.down with b32 type
            // Format: shfl.sync.down.b32 dst, src, delta, clamp, membermask;
            s.push_str("shfl.sync.down.b32");
        }
        PtxOp::ShflIdx => {
            // sm_70+ requires shfl.sync.idx with b32 type
            // Format: shfl.sync.idx.b32 dst, src, srcLane, clamp, membermask;
            s.push_str("shfl.sync.idx.b32");
        }
        PtxOp::Ex2 => {
            // ex2 requires .approx modifier for f32
            s.push_str("ex2.approx");
        }
        _ => s.push_str(&format!("{:?}", instr.op).to_lowercase()),
    }

    // Type suffix (skip for Cvt, wide Mul, Fma, ShflDown which handle types specially)
    // NOTE: mul.lo.u64 still needs .u64 suffix, only mul.wide.u32 skips it
    let is_wide_mul_from_u32 = instr.op == PtxOp::Mul
        && (instr.ty == PtxType::U64 || instr.ty == PtxType::S64)
        && !instr.srcs.first().is_some_and(|src| {
            matches!(src, Operand::Reg(vreg) if vreg.ty() == PtxType::U64 || vreg.ty() == PtxType::S64)
        });
    let skip_type_suffix =
        instr.op == PtxOp::Cvt || is_wide_mul_from_u32 || instr.op == PtxOp::ShflDown || instr.op == PtxOp::ShflIdx;
    if !skip_type_suffix {
        s.push_str(instr.ty.to_ptx_string());
    }

    s.push(' ');

    // Destination
    if let Some(dst) = &instr.dst {
        s.push_str(&emit_operand(dst));
        if !instr.srcs.is_empty() {
            s.push_str(", ");
        }
    }

    // Sources - handle memory addressing specially for Ld/St
    let is_memory_op = matches!(instr.op, PtxOp::Ld | PtxOp::St);
    let is_shared_mem = instr.state_space == Some(PtxStateSpace::Shared);
    let is_global_mem = instr.state_space == Some(PtxStateSpace::Global)
        || (is_memory_op && instr.state_space.is_none());

    for (i, src) in instr.srcs.iter().enumerate() {
        // For memory ops, first source (address) needs bracket format
        if i == 0 && is_memory_op {
            if is_shared_mem {
                s.push_str(&emit_shared_mem_operand(src));
            } else if is_global_mem {
                s.push_str(&emit_global_mem_operand(src));
            } else {
                s.push_str(&emit_operand(src));
            }
        } else {
            s.push_str(&emit_operand(src));
        }
        if i < instr.srcs.len() - 1 {
            s.push_str(", ");
        }
    }

    s.push_str(";\n");
    s
}

/// Emit shared memory operand with proper addressing syntax
/// For shared memory, we use direct address register (caller computes smem base + offset)
fn emit_shared_mem_operand(op: &Operand) -> String {
    match op {
        Operand::Reg(vreg) => format!("[{}]", vreg.to_ptx_string()),
        Operand::Addr { base, offset } => {
            if *offset == 0 {
                format!("[{}]", base.to_ptx_string())
            } else {
                format!("[{}+{}]", base.to_ptx_string(), offset)
            }
        }
        _ => emit_operand(op),
    }
}

/// Emit global memory operand with proper [addr] syntax
fn emit_global_mem_operand(op: &Operand) -> String {
    match op {
        Operand::Reg(vreg) => format!("[{}]", vreg.to_ptx_string()),
        Operand::Addr { base, offset } => {
            if *offset == 0 {
                format!("[{}]", base.to_ptx_string())
            } else {
                format!("[{}+{}]", base.to_ptx_string(), offset)
            }
        }
        _ => emit_operand(op),
    }
}

/// Emit an operand
fn emit_operand(op: &Operand) -> String {
    match op {
        Operand::Reg(vreg) => vreg.to_ptx_string(),
        Operand::SpecialReg(sreg) => sreg.to_ptx_string().to_string(),
        Operand::ImmI64(v) => v.to_string(),
        Operand::ImmU64(v) => v.to_string(),
        Operand::ImmF32(v) => emit_f32_literal(*v),
        Operand::ImmF64(v) => emit_f64_literal(*v),
        Operand::Param(name) => format!("[{}]", name),
        Operand::Addr { base, offset } => {
            if *offset == 0 {
                format!("[{}]", base.to_ptx_string())
            } else {
                format!("[{}+{}]", base.to_ptx_string(), offset)
            }
        }
        Operand::Label(name) => name.clone(),
    }
}

/// Emit f32 literal in PTX hex format (0Fxxxxxxxx)
fn emit_f32_literal(v: f32) -> String {
    let bits = v.to_bits();
    format!("0F{:08X}", bits)
}

/// Emit f64 literal in PTX hex format (0Dxxxxxxxxxxxxxxxx)
fn emit_f64_literal(v: f64) -> String {
    let bits = v.to_bits();
    format!("0D{:016X}", bits)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_module_defaults() {
        let module = PtxModule::new();
        assert_eq!(module.get_version(), (8, 0));
        assert_eq!(module.get_target(), "sm_70");
        assert_eq!(module.get_address_size(), 64);
    }

    #[test]
    fn test_module_builder() {
        let module = PtxModule::new()
            .version(8, 5)
            .target("sm_86")
            .address_size(64);

        assert_eq!(module.get_version(), (8, 5));
        assert_eq!(module.get_target(), "sm_86");
    }

    #[test]
    fn test_kernel_params() {
        let kernel = PtxKernel::new("test")
            .param(PtxType::U64, "ptr")
            .param(PtxType::U32, "n");

        assert_eq!(kernel.params.len(), 2);
        assert_eq!(kernel.params[0].name, "ptr");
        assert_eq!(kernel.params[1].name, "n");
    }

    #[test]
    fn test_emit_header() {
        let module = PtxModule::new()
            .version(8, 0)
            .target("sm_70")
            .address_size(64);

        let ptx = module.emit();
        assert!(ptx.contains(".version 8.0"));
        assert!(ptx.contains(".target sm_70"));
        assert!(ptx.contains(".address_size 64"));
    }

    #[test]
    fn test_emit_kernel() {
        let kernel = PtxKernel::new("vector_add")
            .param(PtxType::U64, "a")
            .param(PtxType::U64, "b");

        let module = PtxModule::new().add_kernel(kernel);
        let ptx = module.emit();

        assert!(ptx.contains(".visible .entry vector_add"));
        assert!(ptx.contains(".param .u64 a"));
        assert!(ptx.contains(".param .u64 b"));
    }

    // ========================================================================
    // BUG FIX TESTS - EXTREME TDD
    // ========================================================================

    #[test]
    fn test_bar_sync_emission() {
        // BUG: bar.sync was being emitted as "bar.b32 ;" instead of "bar.sync 0;"
        let kernel = PtxKernel::new("test_barrier")
            .build(|ctx| {
                ctx.bar_sync(0);
                ctx.ret();
            });

        let ptx = kernel.emit();
        // Must contain proper bar.sync instruction
        assert!(
            ptx.contains("bar.sync 0"),
            "Expected 'bar.sync 0' but got: {}",
            ptx
        );
        // Must NOT contain the buggy output
        assert!(
            !ptx.contains("bar.b32"),
            "Found buggy 'bar.b32' in: {}",
            ptx
        );
    }

    #[test]
    fn test_cvt_u64_u32_emission() {
        // BUG: cvt was being emitted as "cvt.u64 %r, %r" instead of "cvt.u64.u32 %r, %r"
        let kernel = PtxKernel::new("test_cvt")
            .build(|ctx| {
                let val = ctx.mov_u32_imm(42);
                let _wide = ctx.cvt_u64_u32(val);
                ctx.ret();
            });

        let ptx = kernel.emit();
        // Must contain proper cvt with both types
        assert!(
            ptx.contains("cvt.u64.u32"),
            "Expected 'cvt.u64.u32' but got: {}",
            ptx
        );
    }

    #[test]
    fn test_shared_memory_addressing() {
        // Shared memory access uses register-based addressing
        let kernel = PtxKernel::new("test_shared")
            .shared_memory(1024)
            .build(|ctx| {
                let val = ctx.mov_f32_imm(1.0);
                let offset = ctx.mov_u32_imm(0);
                let offset_64 = ctx.cvt_u64_u32(offset);
                ctx.st_shared_f32(offset_64, val);
                let _loaded = ctx.ld_shared_f32(offset_64);
                ctx.ret();
            });

        let ptx = kernel.emit();
        // Must contain proper shared memory operations
        assert!(
            ptx.contains("st.shared.f32") && ptx.contains("ld.shared.f32"),
            "Expected shared memory operations, got: {}",
            ptx
        );
        // Must contain brackets for addressing
        assert!(
            ptx.contains("[%rd"),
            "Expected bracketed register address, got: {}",
            ptx
        );
    }

    #[test]
    fn test_bar_sync_with_different_barriers() {
        // Test barrier with different IDs
        let kernel = PtxKernel::new("test_barriers")
            .build(|ctx| {
                ctx.bar_sync(0);
                ctx.bar_sync(1);
                ctx.ret();
            });

        let ptx = kernel.emit();
        assert!(
            ptx.contains("bar.sync 0"),
            "Expected 'bar.sync 0' in: {}",
            ptx
        );
        assert!(
            ptx.contains("bar.sync 1"),
            "Expected 'bar.sync 1' in: {}",
            ptx
        );
    }

    #[test]
    fn test_global_memory_addressing() {
        // BUG: global memory access was "ld.global.f32 %f, %r" instead of "ld.global.f32 %f, [%r]"
        let kernel = PtxKernel::new("test_global")
            .param(PtxType::U64, "ptr")
            .build(|ctx| {
                let ptr = ctx.load_param_u64("ptr");
                let val = ctx.ld_global_f32(ptr);
                ctx.st_global_f32(ptr, val);
                ctx.ret();
            });

        let ptx = kernel.emit();
        // Load must use brackets for address
        assert!(
            ptx.contains("ld.global.f32") && ptx.contains("[%rd"),
            "Expected ld.global.f32 with [%rd] address, got: {}",
            ptx
        );
        // Store must use brackets for address
        assert!(
            ptx.contains("st.global.f32 ["),
            "Expected st.global.f32 with [%rd] address, got: {}",
            ptx
        );
    }

    #[test]
    fn test_f32_literal_format() {
        // BUG: float literals were emitted as "0e0" instead of PTX hex format "0F00000000"
        let kernel = PtxKernel::new("test_float")
            .build(|ctx| {
                let _zero = ctx.mov_f32_imm(0.0);
                let _one = ctx.mov_f32_imm(1.0);
                ctx.ret();
            });

        let ptx = kernel.emit();
        // PTX float literals must be in hex format
        assert!(
            ptx.contains("0F00000000"), // 0.0f in hex
            "Expected 0F00000000 for 0.0f, got: {}",
            ptx
        );
        assert!(
            ptx.contains("0F3F800000"), // 1.0f in hex
            "Expected 0F3F800000 for 1.0f, got: {}",
            ptx
        );
    }

    #[test]
    fn test_loop_counter_update_in_place() {
        // BUG: Loop counters were never updated due to SSA - computed values discarded
        // PTX loops need in-place register updates: add.u32 %r0, %r0, 1
        let kernel = PtxKernel::new("test_loop")
            .param(PtxType::U32, "n")
            .build(|ctx| {
                let n = ctx.load_param_u32("n");
                let i = ctx.mov_u32_imm(0);
                ctx.label("loop");
                let done = ctx.setp_ge_u32(i, n);
                ctx.branch_if(done, "exit");
                // In-place increment: i = i + 1
                ctx.add_u32_inplace(i, 1);
                ctx.branch("loop");
                ctx.label("exit");
                ctx.ret();
            });

        let ptx = kernel.emit();
        // The loop counter must be updated in-place (same src and dst register)
        // Look for pattern like: add.u32 %r1, %r1, 1
        assert!(
            ptx.contains("add") && ptx.contains("%r") && ptx.contains(", 1"),
            "Expected in-place add instruction, got: {}",
            ptx
        );
    }

    #[test]
    fn test_accumulator_update_in_place() {
        // BUG: Accumulators in inner loops were never updated
        // Need in-place: add.f32 %f0, %f0, %f1
        let kernel = PtxKernel::new("test_acc")
            .build(|ctx| {
                let acc = ctx.mov_f32_imm(0.0);
                let val = ctx.mov_f32_imm(1.0);
                // In-place accumulate: acc = acc + val
                ctx.add_f32_inplace(acc, val);
                ctx.ret();
            });

        let ptx = kernel.emit();
        // Must have in-place add
        assert!(
            ptx.contains("add") && ptx.contains(".f32"),
            "Expected f32 add instruction, got: {}",
            ptx
        );
    }

    // ========================================================================
    // TENSOR CORE (WMMA) TESTS - IMP-1000a
    // ========================================================================

    #[test]
    fn test_wmma_load_a_f16() {
        let kernel = PtxKernel::new("test_wmma_load_a")
            .param(PtxType::U64, "a_ptr")
            .build(|ctx| {
                let ptr = ctx.load_param_u64("a_ptr");
                let _frag_a = ctx.wmma_load_a_f16(ptr, 16, WmmaLayout::RowMajor);
                ctx.ret();
            });

        let ptx = kernel.emit();
        assert!(
            ptx.contains(".param .u64 a_ptr"),
            "Expected a_ptr param, got: {}",
            ptx
        );
    }

    #[test]
    fn test_wmma_load_b_f16() {
        let kernel = PtxKernel::new("test_wmma_load_b")
            .param(PtxType::U64, "b_ptr")
            .build(|ctx| {
                let ptr = ctx.load_param_u64("b_ptr");
                let _frag_b = ctx.wmma_load_b_f16(ptr, 16, WmmaLayout::ColMajor);
                ctx.ret();
            });

        let ptx = kernel.emit();
        assert!(
            ptx.contains(".param .u64 b_ptr"),
            "Expected b_ptr param, got: {}",
            ptx
        );
    }

    #[test]
    fn test_wmma_mma_f16_f32() {
        let kernel = PtxKernel::new("test_wmma_mma")
            .param(PtxType::U64, "a_ptr")
            .param(PtxType::U64, "b_ptr")
            .param(PtxType::U64, "c_ptr")
            .build(|ctx| {
                let a = ctx.load_param_u64("a_ptr");
                let b = ctx.load_param_u64("b_ptr");
                let c = ctx.load_param_u64("c_ptr");

                let frag_a = ctx.wmma_load_a_f16(a, 16, WmmaLayout::RowMajor);
                let frag_b = ctx.wmma_load_b_f16(b, 16, WmmaLayout::ColMajor);
                let frag_c = ctx.wmma_load_c_f32(c, 16, WmmaLayout::RowMajor);

                let _frag_d = ctx.wmma_mma_f16_f32(&frag_a, &frag_b, &frag_c);
                ctx.ret();
            });

        let ptx = kernel.emit();
        // Verify kernel structure
        assert!(
            ptx.contains(".visible .entry test_wmma_mma"),
            "Expected kernel entry, got: {}",
            ptx
        );
    }

    #[test]
    fn test_wmma_store_d_f32() {
        let kernel = PtxKernel::new("test_wmma_store")
            .param(PtxType::U64, "d_ptr")
            .build(|ctx| {
                let d = ctx.load_param_u64("d_ptr");
                // Create empty fragment for test
                let frag_d = vec![ctx.mov_f32_imm(0.0)];
                ctx.wmma_store_d_f32(d, &frag_d, 16, WmmaLayout::RowMajor);
                ctx.ret();
            });

        let ptx = kernel.emit();
        assert!(
            ptx.contains(".param .u64 d_ptr"),
            "Expected d_ptr param, got: {}",
            ptx
        );
    }

    #[test]
    fn test_cvt_f16_f32() {
        let kernel = PtxKernel::new("test_cvt_f16")
            .build(|ctx| {
                let f32_val = ctx.mov_f32_imm(1.5);
                let _f16_val = ctx.cvt_f16_f32(f32_val);
                ctx.ret();
            });

        let ptx = kernel.emit();
        assert!(
            ptx.contains("cvt"),
            "Expected cvt instruction, got: {}",
            ptx
        );
    }

    #[test]
    fn test_cvt_f32_f16() {
        let kernel = PtxKernel::new("test_cvt_f32")
            .param(PtxType::U64, "ptr")
            .build(|ctx| {
                let ptr = ctx.load_param_u64("ptr");
                let f16_val = ctx.ld_global_f16(ptr);
                let _f32_val = ctx.cvt_f32_f16(f16_val);
                ctx.ret();
            });

        let ptx = kernel.emit();
        assert!(
            ptx.contains(".param .u64 ptr"),
            "Expected ptr param, got: {}",
            ptx
        );
    }

    #[test]
    fn test_ld_st_global_f16() {
        let kernel = PtxKernel::new("test_f16_mem")
            .param(PtxType::U64, "in_ptr")
            .param(PtxType::U64, "out_ptr")
            .build(|ctx| {
                let in_ptr = ctx.load_param_u64("in_ptr");
                let out_ptr = ctx.load_param_u64("out_ptr");
                let val = ctx.ld_global_f16(in_ptr);
                ctx.st_global_f16(out_ptr, val);
                ctx.ret();
            });

        let ptx = kernel.emit();
        assert!(
            ptx.contains(".param .u64 in_ptr") && ptx.contains(".param .u64 out_ptr"),
            "Expected both params, got: {}",
            ptx
        );
    }
}
