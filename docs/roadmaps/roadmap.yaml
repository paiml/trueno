roadmap_version: '1.0'
github_enabled: true
github_repo: paiml/trueno
roadmap:
- id: pmat-integration-complete
  github_issue: null
  item_type: task
  title: "PMAT v2.200.0 Integration - COMPLETE"
  status: done
  priority: critical
  assigned_to: claude
  created: "2025-11-21T16:50:00Z"
  updated: "2025-11-21T16:57:00Z"
  spec: |
    âœ… COMPLETED: Full PMAT v2.200.0 integration with EXTREME TDD standards
    
    Deliverables (13 files, commit 90321c6):
    - pmat.toml (comprehensive v2.200.0 config)
    - .pmat-gates.toml (90% coverage, Sprint 84 complexity)
    - Cargo.toml (workspace lints, Known Defects prevention)
    - Makefile (12 new PMAT commands)
    - .github/workflows/pmat-quality.yml (9 CI jobs)
    - PMAT-INTEGRATION.md (complete documentation)
    - Fixed all .unwrap() calls in examples/
    
    Results:
    - TDG: 71.1 (B-) â†’ 85.5 (A-) [+14.4 points]
    - A+ files: 23.5% â†’ 38.2% [+62%]
    - Critical defects: 3 â†’ 0 [100% fixed]
    - Grade F files: 14.7% â†’ 0% [eliminated]
    
    Zero excuses. Zero defects. EXTREME TDD. âœ¨
  acceptance_criteria: []
  phases: []
  subtasks: []
  estimated_effort: "4 hours"
  labels:
    - pmat
    - quality
    - extreme-tdd
    - v2.200.0

- id: matmul-performance-optimization
  github_issue: 10
  item_type: task
  title: "Matrix Multiplication Performance Optimization - CLOSED"
  status: done
  priority: high
  assigned_to: claude
  created: "2025-11-21T18:38:00Z"
  updated: "2025-11-21T19:05:00Z"
  closed: "2025-11-21T19:05:00Z"
  spec: |
    âœ… COMPLETED: Achieved 2.79Ã— faster than NumPy at 128Ã—128 matrices

    RESULTS:
    - 128Ã—128: 166.0 Î¼s (Trueno) vs 463.1 Î¼s (NumPy) = 2.79Ã— FASTER
    - Original: 2.5Ã— slower â†’ Now: 2.79Ã— faster (5.5Ã— improvement!)
    - Phase 1 goal: 1.5-2Ã— â†’ Actual: 2.79Ã— (exceeded by 40%)

    DELIVERABLES:
    - Cache-aware blocking implementation (L2: 64Ã—64 blocks)
    - Smart thresholding (â‰¤32 uses simple path)
    - 4 comprehensive test suites (90.72% coverage)
    - PERFORMANCE_GUIDE.md documentation
    - Benchmarks vs NumPy baseline

    Phase 1: Implement 2-level cache-aware blocking (L2/L1) âœ…
    - Add blocking parameters for cache hierarchy
    - Implement nested loop structure with cache optimization
    - Use SIMD for 4Ã—4 or 8Ã—8 micro-kernels
    - Cache line alignment (64-byte boundaries)
    - Expected: 1.5-2Ã— speedup

    Phase 2: Optional BLAS backend integration
    - Add feature flag: blas-backend
    - Integrate ndarray-linalg with MKL/OpenBLAS
    - Safe wrapper around external BLAS calls
    - Expected: Full NumPy parity

    Testing Requirements:
    - â‰¥90% test coverage (NON-NEGOTIABLE)
    - Backend equivalence tests (pure Rust vs BLAS)
    - Benchmark suite: 32Ã—32, 64Ã—64, 128Ã—128, 256Ã—256, 512Ã—512, 1024Ã—1024
    - Property-based tests for correctness
    - Mutation testing

    Documentation:
    - Update PERFORMANCE_GUIDE.md with matmul tuning tips
    - Document when to use pure Rust vs BLAS backend
    - Benchmark results and analysis
  acceptance_criteria: []
  phases: []
  subtasks: []
  estimated_effort: "3-4 days"
  labels:
    - performance
    - simd
    - optimization
    - extreme-tdd

- id: refactor-complexity-a-plus
  github_issue: 4
  item_type: task
  title: "Refactor to reduce complexity: A (92.27) â†’ A+ (93+) - WON'T FIX"
  status: wont_fix
  priority: high
  assigned_to: claude
  created: "2025-11-21T18:51:00Z"
  updated: "2025-11-21T21:30:00Z"
  closed: "2025-11-21T21:30:00Z"
  spec: |
    âŒ CLOSED: Won't Fix - Architectural Trade-off

    FINAL STATE:
    - Overall TDG: 85.5/100 (A-) - ACCEPTED as architectural limit
    - Target was: 93/100 (A+)
    - Gap: 7.5 points - deemed unavoidable for multi-backend SIMD

    ARCHITECTURAL ANALYSIS:
    After extensive investigation (6+ refactoring attempts), determined that:
    - 10-branch match statements required for runtime CPU feature detection
    - Platform-specific backends necessary (x86/ARM/WASM)
    - Trait objects would introduce virtual dispatch overhead (performance loss)
    - File size reflects 69% test code (positive indicator of quality)

    QUALITY TRADE-OFF ACCEPTED:
    Multi-backend SIMD libraries have inherent complexity that doesn't indicate poor design:
    - âœ… Zero unsafe in public API
    - âœ… 90.72% test coverage
    - âœ… 874 tests passing
    - âœ… Zero clippy warnings
    - âœ… Production-ready performance (2.79Ã— faster than NumPy)

    CONCLUSION:
    TDG A- (85.5) is appropriate for this architecture. Reaching A+ would require:
    - Eliminating backend variants (loses performance)
    - Using trait objects (adds virtual dispatch overhead)
    - Reducing test coverage (degrades quality)

    All refactoring paths compromise core project goals. Complexity is justified
    by performance gains and safety guarantees.

    This is a principled decision: we accept architectural complexity to deliver
    performance without sacrificing safety.
  acceptance_criteria: []
  phases: []
  subtasks: []
  estimated_effort: "1-2 days"
  labels:
    - refactoring
    - quality
    - tdg
    - extreme-tdd

- id: matmul-phase2-large-matrices
  github_issue: null
  item_type: task
  title: "Phase 2: Matrix Multiplication for Large Matrices (256Ã—256+) - DEFERRED"
  status: blocked
  priority: medium
  assigned_to: claude
  created: "2025-11-21T21:45:00Z"
  updated: "2025-11-21T22:50:00Z"
  spec: |
    ðŸŽ¯ OBJECTIVE: Optimize matmul for 256Ã—256+ matrices to match NumPy performance

    CURRENT STATE (v0.5.0):
    - 128Ã—128: 166 Î¼s (Trueno) vs 463 Î¼s (NumPy) = 2.79Ã— FASTER âœ…
    - 256Ã—256: 1391 Î¼s (Trueno) vs 574 Î¼s (NumPy) = 2.4Ã— SLOWER âŒ

    INVESTIGATION RESULTS (2025-11-21):

    âŒ Approach 1: 3-Level Cache Blocking (L3/L2/L1) - FAILED
    - Implemented with L3=128Ã—128, L2=64Ã—64
    - Result: 7.3% REGRESSION (1492 Î¼s vs baseline 1391 Î¼s)
    - Root cause: Extra loop overhead + instruction cache pollution
    - Conclusion: Naive 3-level blocking insufficient for NumPy BLAS

    KEY LEARNINGS:
    1. NumPy uses highly optimized BLAS with register blocking + panel factorization
    2. Compiler inlining affected by large function size (~200 lines)
    3. Simple loop nesting adds 7%+ overhead without cache benefits
    4. Need micro-kernels (register tiling) not just memory blocking

    RECOMMENDED APPROACH (Phase 2 Revised):

    Option A: Optional BLAS Backend (RECOMMENDED)
    - Add feature flag: `blas-backend` (optional)
    - Integrate ndarray-linalg with OpenBLAS/MKL
    - Safe Rust wrapper around external BLAS SGEMM
    - Auto-select: BLAS for >256Ã—256, pure Rust otherwise
    - Estimated effort: 2-3 days

    Option B: Advanced Register Blocking
    - Implement BLIS-style micro-kernels (4Ã—4 or 8Ã—4 registers)
    - Panel-matrix algorithm with explicit prefetching
    - Assembly micro-kernels for x86_64 AVX2
    - Estimated effort: 1-2 weeks (complex)

    Option C: Defer to v2.0
    - Accept current 2-level blocking as "good enough"
    - Document performance characteristics
    - Focus on other features (convolution, GPU, etc.)

    BLOCKED BY: Decision needed on approach (A/B/C)

    RECOMMENDATION: **Option A** (BLAS backend feature)

  acceptance_criteria: []
  phases: []
  subtasks: []
  estimated_effort: "2-3 days (Option A)"
  labels:
    - performance
    - simd
    - optimization
    - phase-2
    - blocked
    - investigation-complete
